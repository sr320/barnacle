{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import tensorly as tl\n",
    "from tensorly import check_random_state\n",
    "from tensorly.cp_tensor import CPTensor\n",
    "from barnacle import (\n",
    "    SparseCP,    \n",
    "    visualize_3d_tensor, \n",
    "    plot_factors_heatmap, \n",
    "    simulated_sparse_tensor, \n",
    "    pairs_precision_recall\n",
    ")\n",
    "import tlviz\n",
    "from tlviz.factor_tools import factor_match_score, cosine_similarity\n",
    "from tlviz.model_evaluation import relative_sse\n",
    "from sklearn.model_selection import ParameterGrid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "\n",
    "# function to calculate f1 score from composite precision & recall scores\n",
    "def composite_f1(precision, recall):\n",
    "    '''\n",
    "    Calculates F1 score from precision and recall.'''\n",
    "    numerator = precision + recall\n",
    "    if numerator == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (2 * precision * recall) / numerator\n",
    "\n",
    "# functions to build continuous colormap\n",
    "# source: https://towardsdatascience.com/beautiful-custom-colormaps-with-matplotlib-5bab3d1f0e72\n",
    "\n",
    "def hex_to_rgb(value):\n",
    "    '''\n",
    "    Converts hex to rgb colours\n",
    "    value: string of 6 characters representing a hex colour.\n",
    "    Returns: list length 3 of RGB values'''\n",
    "    value = value.strip(\"#\") # removes hash symbol if present\n",
    "    lv = len(value)\n",
    "    return tuple(int(value[i:i + lv // 3], 16) for i in range(0, lv, lv // 3))\n",
    "\n",
    "\n",
    "def rgb_to_dec(value):\n",
    "    '''\n",
    "    Converts rgb to decimal colours (i.e. divides each value by 256)\n",
    "    value: list (length 3) of RGB values\n",
    "    Returns: list (length 3) of decimal values'''\n",
    "    return [v/256 for v in value]\n",
    "\n",
    "def get_continuous_cmap(hex_list, float_list=None):\n",
    "    ''' creates and returns a color map that can be used in heat map figures.\n",
    "        If float_list is not provided, colour map graduates linearly between each color in hex_list.\n",
    "        If float_list is provided, each color in hex_list is mapped to the respective location in float_list. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hex_list: list of hex code strings\n",
    "        float_list: list of floats between 0 and 1, same length as hex_list. Must start with 0 and end with 1.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        colour map'''\n",
    "    rgb_list = [rgb_to_dec(hex_to_rgb(i)) for i in hex_list]\n",
    "    if float_list:\n",
    "        pass\n",
    "    else:\n",
    "        float_list = list(np.linspace(0,1,len(rgb_list)))\n",
    "        \n",
    "    cdict = dict()\n",
    "    for num, col in enumerate(['red', 'green', 'blue']):\n",
    "        col_list = [[float_list[i], rgb_list[i][num], rgb_list[i][num]] for i in range(len(float_list))]\n",
    "        cdict[col] = col_list\n",
    "    cmp = mcolors.LinearSegmentedColormap('my_cmp', segmentdata=cdict, N=256)\n",
    "    return cmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set aesthetic parameters\n",
    "\n",
    "line_color = '#0F0A0A'    # dark\n",
    "# line_color = '#E5E4E2'    # light\n",
    "\n",
    "neutral_color = '#E0E0E0'\n",
    "\n",
    "# thursday\n",
    "accent_colors = ['#9B5DE5', '#FFAC69', '#00C9AE', '#FD3F92', '#0F0A0A', \n",
    "                 '#959AB1', '#FFDB66', '#63B9FF','#FFB1CA', '#4F1DD7']\n",
    "\n",
    "grays = ['#0F0A0A', '#52525E', '#747688', '#959AB1', '#959AB1', '#CECFD5', '#E0E0E0']\n",
    "\n",
    "style = {'axes.edgecolor': line_color,\n",
    "         'axes.labelcolor': line_color,\n",
    "         'text.color': line_color,\n",
    "         'xtick.color': line_color,\n",
    "         'ytick.color': line_color,\n",
    "         'font.family': 'Helvetica',\n",
    "         'font.Helvetica': ['Helvetica']}\n",
    "\n",
    "palette = sns.color_palette(accent_colors)\n",
    "\n",
    "sns.set_context('talk', rc={'lines.linewidth': 2})\n",
    "sns.set_palette(palette)\n",
    "# sns.set_palette('tab20')\n",
    "sns.set_style('ticks', style)\n",
    "\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "plt.rcParams['axes.spines.left'] = True\n",
    "plt.rcParams['axes.spines.right'] = True\n",
    "plt.rcParams['axes.spines.top'] = True\n",
    "plt.rcParams['axes.spines.bottom'] = True\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "plt.rcParams['patch.linewidth'] = 0\n",
    "plt.rcParams['patch.edgecolor'] = 'none'\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "div_hexes = ['#7222D3', neutral_color, '#FF780A']\n",
    "div_cmap = get_continuous_cmap(div_hexes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# generate simulated data tensor for visualization\n",
    "\n",
    "true_rank = 8\n",
    "true_shape = [50, 30, 20]\n",
    "true_densities = [.2, .2, .2]\n",
    "\n",
    "# re-seed simulated data until all factor matrices are full rank\n",
    "full_rank = False\n",
    "while not full_rank:\n",
    "    # generate simulated tensor\n",
    "    sim_tensor = simulated_sparse_tensor(\n",
    "        shape=true_shape,                \n",
    "        rank=true_rank,                         \n",
    "        densities=true_densities, \n",
    "        factor_dist_list=[scipy.stats.uniform(loc=-1, scale=2), \n",
    "                         scipy.stats.uniform(), \n",
    "                         scipy.stats.uniform()], \n",
    "        random_state=1991\n",
    "    )\n",
    "    # check that all factors are full rank\n",
    "    full_rank = np.all([np.linalg.matrix_rank(factor) == true_rank for factor in sim_tensor.factors])\n",
    "\n",
    "# Ensure that factor matrices are full rank\n",
    "for factor in sim_tensor.factors:\n",
    "    print(np.linalg.matrix_rank(factor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# visualize simulated tensor data\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig \u001b[38;5;241m=\u001b[39m visualize_3d_tensor(\n\u001b[1;32m      4\u001b[0m     sim_tensor\u001b[38;5;241m.\u001b[39mto_tensor(), \n\u001b[1;32m      5\u001b[0m     shell\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     opacity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 17\u001b[0m \u001b[43mfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfigures/1-simulation/example-tensor.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m fig\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/miniconda3/envs/barnacle311/lib/python3.11/site-packages/plotly/basedatatypes.py:3841\u001b[0m, in \u001b[0;36mBaseFigure.write_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3781\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3782\u001b[0m \u001b[38;5;124;03mConvert a figure to a static image and write it to a file or writeable\u001b[39;00m\n\u001b[1;32m   3783\u001b[0m \u001b[38;5;124;03mobject\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3837\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[1;32m   3838\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3839\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpio\u001b[39;00m\n\u001b[0;32m-> 3841\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/barnacle311/lib/python3.11/site-packages/plotly/io/_kaleido.py:266\u001b[0m, in \u001b[0;36mwrite_image\u001b[0;34m(fig, file, format, scale, width, height, validate, engine)\u001b[0m\n\u001b[1;32m    250\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    251\u001b[0m \u001b[38;5;250m                \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03mCannot infer image type from output path '{file}'.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m                 )\n\u001b[1;32m    261\u001b[0m             )\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;66;03m# Request image\u001b[39;00m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;66;03m# -------------\u001b[39;00m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;66;03m# Do this first so we don't create a file if image conversion fails\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m     img_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_image\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;66;03m# Open file\u001b[39;00m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;66;03m# ---------\u001b[39;00m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;66;03m# We previously failed to make sense of `file` as a pathlib object.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;66;03m# Attempt to write to `file` as an open file descriptor.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/barnacle311/lib/python3.11/site-packages/plotly/io/_kaleido.py:132\u001b[0m, in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;66;03m# Raise informative error message if Kaleido is not installed\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scope \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m \u001b[38;5;250m            \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mImage export using the \"kaleido\" engine requires the kaleido package,\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03mwhich can be installed using pip:\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;03m    $ pip install -U kaleido\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# Validate figure\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# ---------------\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     fig_dict \u001b[38;5;241m=\u001b[39m validate_coerce_fig_to_dict(fig, validate)\n",
      "\u001b[0;31mValueError\u001b[0m: \nImage export using the \"kaleido\" engine requires the kaleido package,\nwhich can be installed using pip:\n    $ pip install -U kaleido\n"
     ]
    }
   ],
   "source": [
    "# visualize simulated tensor data\n",
    "\n",
    "fig = visualize_3d_tensor(\n",
    "    sim_tensor.to_tensor(), \n",
    "    shell=False, \n",
    "    midpoint=0, \n",
    "    bg_color='rgba(0, 0, 0, 0)', \n",
    "    show_colorbar=False,\n",
    "    label_axes=False, \n",
    "    figure_kwargs={\n",
    "        'color_continuous_scale': div_hexes, \n",
    "    }, \n",
    "    range_color=[-1, 1], \n",
    "    opacity=1\n",
    ")\n",
    "\n",
    "fig.write_image('figures/1-simulation/example-tensor.png', scale=3)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize components\n",
    "\n",
    "for i, j in enumerate([0, 1, 3]):\n",
    "    component = sim_tensor.get_components()[j]\n",
    "    fig = visualize_3d_tensor(\n",
    "        component.to_tensor(), \n",
    "        shell=False, \n",
    "        midpoint=0, \n",
    "        bg_color='rgba(0, 0, 0, 0)', \n",
    "        show_colorbar=False,\n",
    "        label_axes=False, \n",
    "        figure_kwargs={\n",
    "            'color_continuous_scale': div_hexes, \n",
    "        }, \n",
    "        range_color=[-1, 1], \n",
    "        opacity=1\n",
    "    )\n",
    "    fig.write_image('figures/1-simulation/example-component-{}.png'.format(i), scale=3)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot factor heatmaps\n",
    "\n",
    "heatmap_params = {'vmin':-1, 'vmax':1, 'cmap':div_cmap, 'center':0}\n",
    "fig, ax = plot_factors_heatmap(\n",
    "    tl.cp_normalize(sim_tensor).factors, \n",
    "    mask_thold=[0, 0], \n",
    "    ratios=True, \n",
    "    heatmap_kwargs=heatmap_params\n",
    ")\n",
    "fig.savefig('figures/1-simulation/example-factors.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate simulated data for example test figure\n",
    "\n",
    "# seed & random state\n",
    "seed = 9481\n",
    "rns = check_random_state(seed)\n",
    "\n",
    "# parameters\n",
    "true_rank = 8\n",
    "true_shape = [50, 20, 30]\n",
    "true_densities = [.4, .8, .6]\n",
    "\n",
    "# re-seed simulated data until all factor matrices are full rank\n",
    "full_rank = False\n",
    "while not full_rank:\n",
    "    # generate simulated tensor\n",
    "    sim_tensor = simulated_sparse_tensor(\n",
    "        shape=true_shape,                \n",
    "        rank=true_rank,                         \n",
    "        densities=true_densities, \n",
    "        factor_dist_list=[scipy.stats.uniform(loc=-1, scale=2), \n",
    "                         scipy.stats.uniform(), \n",
    "                         scipy.stats.uniform()], \n",
    "        random_state=rns\n",
    "    )\n",
    "    # check that all factors are full rank\n",
    "    full_rank = np.all([np.linalg.matrix_rank(factor) == true_rank for factor in sim_tensor.factors])\n",
    "\n",
    "# Ensure that factor matrices are full rank\n",
    "for factor in sim_tensor.factors:\n",
    "    print(np.linalg.matrix_rank(factor))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1a\n",
    "\n",
    "Reconstruction error as a function of rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Decomposition with range of components and l1 coefficient manually tuned \n",
    "\n",
    "# model parameters\n",
    "replicates = ['A']\n",
    "noise_level = 2.0\n",
    "model_params = {\n",
    "    'rank': [int(i) for i in np.linspace(1, 12, 12)], \n",
    "    'lambdas': [[i, 0.0, 0.0] for i in [0.0, 0.1, 1.0, 10.0, 100.]], \n",
    "    'nonneg_modes': [[1, 2]],\n",
    "    'tol': [1e-5], \n",
    "    'n_iter_max': [2000], \n",
    "    'n_initializations': [5]\n",
    "}\n",
    "param_grid = list(ParameterGrid(model_params))\n",
    "\n",
    "# make noisy simulated tensors\n",
    "tensors = {}\n",
    "for rep in replicates:\n",
    "    tensors[rep] = sim_tensor.to_tensor(noise_level=noise_level, sparse_noise=True, random_state=rns)\n",
    "\n",
    "results = []\n",
    "# iterate through decompositions\n",
    "for i, params in enumerate(param_grid):\n",
    "    print('Decomposition {} of {}: {}'.format(i, len(param_grid), params))\n",
    "    # update params to reflect accurate normalization\n",
    "    if not np.any(params['lambdas']):\n",
    "        params['norm_constraint'] = False\n",
    "    # fit model to each replicate\n",
    "    cps = {}\n",
    "    for rep in replicates:\n",
    "        cps[rep] = SparseCP(**params, random_state=rns).fit_transform(tensors[rep], threads=1, verbose=0)\n",
    "    # calculate cross-validated and ground truth error\n",
    "    for modeled_rep in replicates:\n",
    "        for comparison_rep in replicates:\n",
    "            # calculate cross-validated SSE\n",
    "            if modeled_rep != comparison_rep:\n",
    "                sse = relative_sse(cps[modeled_rep], tensors[comparison_rep])\n",
    "                comparison = 'cross-validation'\n",
    "            # calculate ground truth comparison SSE\n",
    "            elif modeled_rep == comparison_rep:\n",
    "                sse = relative_sse(cps[modeled_rep], sim_tensor.to_tensor())\n",
    "                comparison = 'ground truth'\n",
    "            # append results\n",
    "            results.append({\n",
    "                'modeled_replicate': modeled_rep, \n",
    "                'comparison_replicate': comparison_rep, \n",
    "                'comparison': comparison, \n",
    "                'rank': params['rank'], \n",
    "                'lambda': params['lambdas'][0], \n",
    "                'SSE': sse\n",
    "            })\n",
    "                \n",
    "\n",
    "rank_df = pd.DataFrame(results)\n",
    "rank_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot SSE vs. rank by λ\n",
    "\n",
    "rank_df['sparsity coefficient'] = ['{}'.format(l) for l in rank_df['lambda']]\n",
    "\n",
    "fig, axis = plt.subplots(figsize=(7, 5))\n",
    "fig = sns.lineplot(\n",
    "    x='rank', \n",
    "    y='SSE', \n",
    "    color=sns.color_palette()[4], \n",
    "    style='sparsity coefficient', \n",
    "    data=rank_df, \n",
    "    ax=axis\n",
    ")\n",
    "plt.vlines(x=8, ymin=0.0, ymax=1, colors='white', linestyles='dotted', label=' ')    # hack for blank space in legend\n",
    "plt.vlines(x=8, ymin=0.0, ymax=1, colors=accent_colors[1], linestyles='dotted', label='true number of\\ncomponents')\n",
    "plt.legend(title='Sparsity Coefficient', loc='center left', bbox_to_anchor=[1, 0.5]);\n",
    "plt.xlabel('number of components');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1b\n",
    "\n",
    "Reconstruction error and fms as a function of lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition with range of components and l1 coefficient manually tuned \n",
    "\n",
    "# model parameters\n",
    "replicates = ['A', 'B', 'C']\n",
    "noise_level = 2.\n",
    "model_params = {\n",
    "    'rank': [8],\n",
    "    'lambdas': [[i, 0.0, 0.0] for i in np.logspace(-3, 2, num=20)], \n",
    "    'nonneg_modes': [[1, 2]],\n",
    "    'tol': [1e-5], \n",
    "    'n_iter_max': [2000], \n",
    "    'n_initializations': [5]\n",
    "}\n",
    "param_grid = list(ParameterGrid(model_params))\n",
    "\n",
    "# make noisy simulated tensors\n",
    "tensors = {}\n",
    "for rep in replicates:\n",
    "    tensors[rep] = sim_tensor.to_tensor(noise_level=noise_level, sparse_noise=True, random_state=rns)\n",
    "\n",
    "results = []\n",
    "# iterate through decompositions\n",
    "for i, params in enumerate(param_grid):\n",
    "    print('Decomposition {} of {}: {}'.format(i, len(param_grid), params))\n",
    "    # fit model to each replicate\n",
    "    cps = {}\n",
    "    for rep in replicates:\n",
    "        cps[rep] = SparseCP(**params, random_state=rns).fit_transform(tensors[rep], threads=1, verbose=0)\n",
    "    # calculate cross-validated and ground truth error\n",
    "    for modeled_rep in replicates:\n",
    "        for comparison_rep in replicates:\n",
    "            # calculate ground truth comparison metrics\n",
    "            if modeled_rep == comparison_rep:\n",
    "                comparison = 'ground truth'\n",
    "                sse = relative_sse(cps[modeled_rep], sim_tensor.to_tensor())\n",
    "                fms = factor_match_score(sim_tensor, cps[modeled_rep], consider_weights=False)\n",
    "                clusters_true = sim_tensor.get_clusters(0, boolean=True)\n",
    "                clusters_fit = cps[modeled_rep].get_clusters(0, boolean=True)\n",
    "                precision, recall = pairs_precision_recall(clusters_true, clusters_fit)\n",
    "                f1 = composite_f1(precision, recall)\n",
    "            # calculate cross-validated metrics FMS & SSE\n",
    "            elif modeled_rep < comparison_rep:\n",
    "                comparison = 'cross-validation'\n",
    "                fms = factor_match_score(cps[comparison_rep], cps[modeled_rep], consider_weights=False)\n",
    "                sse = relative_sse(cps[modeled_rep], tensors[comparison_rep])\n",
    "                clusters_comparison = cps[comparison_rep].get_clusters(0, boolean=True)\n",
    "                clusters_modeled = cps[modeled_rep].get_clusters(0, boolean=True)\n",
    "                precision, recall = pairs_precision_recall(clusters_comparison, clusters_modeled)\n",
    "                f1 = composite_f1(precision, recall)\n",
    "            # calculate only cross-validated SSE\n",
    "            elif modeled_rep > comparison_rep:\n",
    "                comparison = 'cross-validation'\n",
    "                sse = relative_sse(cps[modeled_rep], tensors[comparison_rep])\n",
    "                fms = precision = recall = prerec = np.nan\n",
    "            # append results\n",
    "            results.append({\n",
    "                'modeled_replicate': modeled_rep, \n",
    "                'comparison_replicate': comparison_rep, \n",
    "                'comparison': comparison,\n",
    "                'rank': params['rank'], \n",
    "                'lambda': params['lambdas'][0], \n",
    "                'SSE': sse, \n",
    "                'FMS': fms, \n",
    "                'Precision': precision,\n",
    "                'Recall': recall, \n",
    "                'F1': f1\n",
    "            })\n",
    "            \n",
    "lambda_df = pd.DataFrame(results)\n",
    "lambda_melt_df = lambda_df.melt(\n",
    "    id_vars=['modeled_replicate', 'comparison_replicate', 'comparison', 'rank', 'lambda'], \n",
    "    value_vars=['SSE', 'FMS', 'Precision', 'Recall', 'F1'], \n",
    "    var_name='metric', \n",
    "    value_name='score'\n",
    ")\n",
    "lambda_melt_df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reconstruction error and fms vs lambda\n",
    "\n",
    "# select out plot data\n",
    "plot_df = lambda_melt_df[lambda_melt_df['comparison'] == 'ground truth']\n",
    "plot_df = plot_df[plot_df['metric'].isin(['SSE', 'FMS'])]\n",
    "\n",
    "# plot factor match score and sse\n",
    "fig, axis = plt.subplots(figsize=(7, 5))\n",
    "sns.lineplot(\n",
    "    x='lambda', \n",
    "    y='score', \n",
    "    hue='metric', \n",
    "    palette=sns.color_palette([accent_colors[i] for i in [4, 3]]), \n",
    "    data=plot_df, \n",
    "    ax=axis, \n",
    "    legend=True\n",
    ")\n",
    "plt.legend(title='Metric', loc='center left', bbox_to_anchor=[1, 0.5])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Sparsity Coefficient')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1c\n",
    "\n",
    "Metrics comparing clusters to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select out plot data\n",
    "plot_df = lambda_melt_df[lambda_melt_df['comparison'] == 'ground truth']\n",
    "\n",
    "# plot factor match score and sse\n",
    "fig, axis = plt.subplots(figsize=(7, 5))\n",
    "sns.lineplot(\n",
    "    x='lambda', \n",
    "    y='score', \n",
    "    hue='metric', \n",
    "    palette=sns.color_palette([accent_colors[i] for i in [4, 3, 7, 6, 2]]), \n",
    "    data=plot_df, \n",
    "    ax=axis, \n",
    "    legend=True\n",
    ")\n",
    "plt.legend(title='Metric', loc='center left', bbox_to_anchor=[1, 0.5])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Sparsity Coefficient')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the optima of each metric\n",
    "\n",
    "avg_scores_df = lambda_melt_df[lambda_melt_df['comparison'] == 'ground truth'].groupby(\n",
    "    ['rank', 'lambda', 'metric']).score.mean().reset_index().sort_values(\n",
    "    ['score', 'metric'], ascending=False)\n",
    "\n",
    "for metric in avg_scores_df['metric'].unique():\n",
    "    print('\\n{}'.format(metric))\n",
    "    if metric != 'SSE':\n",
    "        print(avg_scores_df[avg_scores_df['metric'] == metric].iloc[0, :])\n",
    "    else:\n",
    "        print(avg_scores_df[avg_scores_df['metric'] == metric].iloc[-1, :])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 1d\n",
    "\n",
    "Heatmap of optimal model components compared to ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompose with optimal hyperparameters\n",
    "\n",
    "model_params = {\n",
    "    'rank': 8,\n",
    "    'lambdas': [1.44, 0.0, 0.0], \n",
    "    'nonneg_modes': [1, 2],\n",
    "    'tol': 1e-5, \n",
    "    'n_iter_max': 2000, \n",
    "    'n_initializations': 5\n",
    "}\n",
    "noise_level = 2.0\n",
    "\n",
    "# run decomposition\n",
    "cp = SparseCP(**model_params, random_state=rns).fit_transform(\n",
    "    sim_tensor.to_tensor(noise_level=noise_level, sparse_noise=True, random_state=rns), threads=1, verbose=0\n",
    ")\n",
    "\n",
    "# calculate fms and get optimal permutation\n",
    "fms, perm = factor_match_score(sim_tensor, cp, return_permutation=True, allow_smaller_rank=True)\n",
    "sim_index = [i for i, v in enumerate(perm) if type(v) != slice]\n",
    "cp_index = [v for v in perm if type(v) != slice]\n",
    "best_cp = tlviz.factor_tools.permute_cp_tensor(cp, perm)\n",
    "\n",
    "print(fms)\n",
    "\n",
    "# plot factor heatmaps\n",
    "heatmap_params = {'vmin':-1, 'vmax':1, 'cmap':div_cmap, 'center':0}\n",
    "fig, ax = plot_factors_heatmap(\n",
    "    tl.cp_normalize(best_cp).factors, \n",
    "    reference_factors=tl.cp_normalize(sim_tensor).factors, \n",
    "    mask_thold=[0, 0], \n",
    "    ratios=True, \n",
    "    heatmap_kwargs=heatmap_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Figure 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make composite figure\n",
    "\n",
    "fig = plt.figure(figsize=(16, 14))\n",
    "grid = plt.GridSpec(3, 2, wspace=.7, hspace=.4)\n",
    "annot_kwgs = {\n",
    "    'xycoords': 'axes fraction', 'va': 'center', 'ha': 'left', 'fontsize': 22, 'annotation_clip': False\n",
    "}\n",
    "\n",
    "\n",
    "# plot panel a\n",
    "ax_a = plt.subplot(grid[0,0])\n",
    "ax_a.annotate('A', xy=(-0.2, 1.15), **annot_kwgs)\n",
    "# get data together\n",
    "rank_df['sparsity coefficient'] = ['{}'.format(l) for l in rank_df['lambda']]\n",
    "# make figure\n",
    "sns.lineplot(\n",
    "    x='rank', \n",
    "    y='SSE', \n",
    "    style='sparsity coefficient', \n",
    "    color=sns.color_palette()[4], \n",
    "    data=rank_df, \n",
    "    ax=ax_a\n",
    ")\n",
    "ax_a.axvline(x=8, ymin=0, ymax=1, color='white', linestyle='dotted', label=' ')    # hack for blank space in legend\n",
    "ax_a.axvline(x=8, ymin=0, ymax=1, color=accent_colors[1], linestyle='dotted', label='True Number of\\nComponents')\n",
    "ax_a.set(xlim=(0.6, 12.4), xlabel='Number of Components')\n",
    "ax_a.legend(title='Sparsity Coefficient', loc='center left', bbox_to_anchor=[1, 0.5]);\n",
    "\n",
    "\n",
    "# plot panel b\n",
    "ax_b = plt.subplot(grid[1,0])\n",
    "ax_b.annotate('B', xy=(-0.2, 1.15), **annot_kwgs)\n",
    "# select out plot data\n",
    "plot_df = lambda_melt_df[lambda_melt_df['comparison'] == 'ground truth']\n",
    "plot_df = plot_df[plot_df['modeled_replicate'] == 'A']\n",
    "plot_df = plot_df[plot_df['metric'].isin(['SSE', 'FMS'])]\n",
    "# plot factor match score and sse\n",
    "sns.lineplot(\n",
    "    x='lambda', \n",
    "    y='score', \n",
    "    hue='metric', \n",
    "    palette=sns.color_palette([accent_colors[i] for i in [4, 3]]), \n",
    "    data=plot_df, \n",
    "    ax=ax_b, \n",
    ")\n",
    "ax_b.set(xscale='log', xlabel='Sparsity Coefficient', ylabel='Score')\n",
    "ax_b.legend(title='Metric', loc='center left', bbox_to_anchor=[1, 0.5])\n",
    "\n",
    "\n",
    "# plot panel c\n",
    "ax_c = plt.subplot(grid[2,0])\n",
    "ax_c.annotate('C', xy=(-0.2, 1.15), **annot_kwgs)\n",
    "# select out plot data\n",
    "plot_df = lambda_melt_df[lambda_melt_df['comparison'] == 'ground truth']\n",
    "plot_df = plot_df[plot_df['modeled_replicate'] == 'A']\n",
    "# plot factor match score and sse\n",
    "sns.lineplot(\n",
    "    x='lambda', \n",
    "    y='score', \n",
    "    hue='metric', \n",
    "    palette=sns.color_palette([accent_colors[i] for i in [4, 3, 7, 6, 2]]), \n",
    "    data=plot_df, \n",
    "    ax=ax_c, \n",
    ")\n",
    "ax_c.set(xscale='log', xlabel='Sparsity Coefficient', ylabel='Score')\n",
    "ax_c.legend(title='Metric', loc='center left', bbox_to_anchor=[1, 0.5])\n",
    "\n",
    "\n",
    "# plot panel d\n",
    "ax_d = plt.subplot(grid[0:3,1])\n",
    "ax_d.annotate('D', xy=(-0.05, 1.04), **annot_kwgs)\n",
    "# get data in shape\n",
    "cp_factors = tl.cp_normalize(best_cp).factors\n",
    "sim_factors = tl.cp_normalize(sim_tensor).factors\n",
    "data = np.concatenate(\n",
    "    [np.concatenate([sim_factors[0], sim_factors[1], sim_factors[2]]), \n",
    "    np.concatenate([cp_factors[0], cp_factors[1], cp_factors[2]])], \n",
    "    axis=1\n",
    ")\n",
    "# plot heatmap\n",
    "sns.heatmap(\n",
    "    data, \n",
    "    mask=(data == 0), \n",
    "    cbar_kws={'shrink': 0.5, 'label': 'Weight'}, \n",
    "    **heatmap_params, \n",
    "    xticklabels=False,\n",
    "    yticklabels=False,\n",
    "    ax=ax_d, \n",
    ");\n",
    "ax_d.axvline(8, color=line_color);\n",
    "ax_d.axhline(50, color=line_color);\n",
    "ax_d.axhline(70, color=line_color);\n",
    "ax_d.text(4, 103, 'Ground Truth', horizontalalignment='center', verticalalignment='center');\n",
    "ax_d.text(12, 103, 'Model', horizontalalignment='center', verticalalignment='center');\n",
    "ax_d.text(-1, 25, 'Gene Mode', horizontalalignment='center', verticalalignment='center', rotation=90);\n",
    "ax_d.text(-1, 60, 'Taxon Mode', horizontalalignment='center', verticalalignment='center', rotation=90);\n",
    "ax_d.text(-1, 85, 'Sample Mode', horizontalalignment='center', verticalalignment='center', rotation=90);\n",
    "\n",
    "fig.savefig('figures/1-simulation/simulation-example-panel.png', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "barnacle311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
